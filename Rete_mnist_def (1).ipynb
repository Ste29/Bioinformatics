{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Rete_mnist_def.ipynb","provenance":[],"collapsed_sections":["JODP_Q3mw17b","8ct5EUDvyRtk","t89CGIju4hWs","mJa2smRJ_Q8a","egjy0yaMyLe_"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JODP_Q3mw17b","colab_type":"text"},"source":["## Install tf-gpu, imports e download dati, split train val"]},{"cell_type":"code","metadata":{"id":"cIpfOflgDVme","colab_type":"code","outputId":"b313c2e7-6ecf-4494-f218-5054edafc517","executionInfo":{"status":"ok","timestamp":1571664131412,"user_tz":-120,"elapsed":75411,"user":{"displayName":"Stefano Villata","photoUrl":"","userId":"07054100665848574913"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["# tf ha come requisito che gast sia > 0.2, ma l'ultima versione di fast ha un \n","# problema con autograph\n","!pip install gast==0.2.2\n","!pip install tensorflow-gpu==1.14\n","\n","!wget http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/matlab.zip\n","!unzip matlab.zip\n","\n","import os\n","from datetime import timedelta\n","import time\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","\n","tf.logging.set_verbosity(tf.logging.ERROR)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}\n","\n","tfd = tfp.distributions\n","\n","# Controllo della versione di tf, perché da 1/10 quella di default è la 2.0.0\n","print(tf.__version__)  \n","print(tf.test.gpu_device_name())  # Controllo se colab sta usando la gpu\n","\n","# Download dati\n","from keras.datasets import mnist\n","from scipy import io as spio\n","(x_train, y_train), (x_test, y_test) = mnist.load_data() # 60k - 10k\n","\n","# Creo un asse aggiuntivo che rappresenta i colori, nelle immagini bio è 3 \n","# perché sono a colori, qua è 1 perché è a toni di grigio. Senza questo \n","# si blocca sulla rete, cond2dflipout vuole che input gli passi un immagine con \n","# 4 assi (1 per il batch), ma input riceve solo 3 assi da input_pipeline\n","x_train = np.expand_dims(x_train, axis=3)\n","x_test = np.expand_dims(x_test, axis=3)\n","\n","emnist = spio.loadmat(os.path.join(\"matlab\", \"emnist-letters\"))\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (0.2.2)\n","Collecting tensorflow-gpu==1.14\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n","\u001b[K     |████████████████████████████████| 377.0MB 65kB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.1.7)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.16.5)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.11.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.33.6)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.0.8)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.0)\n","Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow-gpu==1.14)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n","\u001b[K     |████████████████████████████████| 491kB 41.7MB/s \n","\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.2.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (3.10.0)\n","Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow-gpu==1.14)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 30.1MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14) (41.2.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (0.16.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.1.1)\n","\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.14.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n","Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n","  Found existing installation: tensorflow-estimator 1.15.1\n","    Uninstalling tensorflow-estimator-1.15.1:\n","      Successfully uninstalled tensorflow-estimator-1.15.1\n","  Found existing installation: tensorboard 1.15.0\n","    Uninstalling tensorboard-1.15.0:\n","      Successfully uninstalled tensorboard-1.15.0\n","Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n","--2019-10-21 13:21:47--  http://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/matlab.zip\n","Resolving www.itl.nist.gov (www.itl.nist.gov)... 129.6.13.51, 2610:20:6b01:4::36\n","Connecting to www.itl.nist.gov (www.itl.nist.gov)|129.6.13.51|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/matlab.zip [following]\n","--2019-10-21 13:21:47--  https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/matlab.zip\n","Connecting to www.itl.nist.gov (www.itl.nist.gov)|129.6.13.51|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 743900280 (709M) [application/zip]\n","Saving to: ‘matlab.zip’\n","\n","matlab.zip          100%[===================>] 709.44M  55.5MB/s    in 12s     \n","\n","2019-10-21 13:21:59 (60.7 MB/s) - ‘matlab.zip’ saved [743900280/743900280]\n","\n","Archive:  matlab.zip\n","  inflating: matlab/emnist-balanced.mat  \n","  inflating: matlab/emnist-byclass.mat  \n","  inflating: matlab/emnist-bymerge.mat  \n","  inflating: matlab/emnist-digits.mat  \n","  inflating: matlab/emnist-letters.mat  \n","  inflating: matlab/emnist-mnist.mat  \n","1.14.0\n","/device:GPU:0\n","Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sl62o8-056py","colab_type":"code","outputId":"b6d2198f-286d-4788-cb89-25df775189eb","executionInfo":{"status":"ok","timestamp":1571664131618,"user_tz":-120,"elapsed":69272,"user":{"displayName":"Stefano Villata","photoUrl":"","userId":"07054100665848574913"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["def split_train_test(x, y, test_ratio):\n","  seed = np.random.RandomState(42)\n","  shuffled_indices  = seed.permutation(len(y))\n","  test_set_size = int(len(y) * test_ratio)\n","  test_indices = shuffled_indices[:test_set_size]\n","  train_indices = shuffled_indices[test_set_size:]\n","  \n","  return (x[train_indices], y[train_indices]), (x[test_indices], y[test_indices])\n","\n","print(len(x_train))\n","(x_train, y_train), (x_val, y_val) = split_train_test(x_train, y_train, 0.15) \n","print(len(x_train))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["60000\n","51000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8ct5EUDvyRtk","colab_type":"text"},"source":["## Parametri"]},{"cell_type":"code","metadata":{"id":"CPnFbuE6yLH8","colab_type":"code","colab":{}},"source":["learning_rate = default=0.0001  # Initial learning rate\n","epochs = 500  #Number of epochs to train for\n","batch_size = 500  #Batch size\n","eval_freq = 400  # Frequency at which to validate the model\n","num_monte_carlo = 50  # Network draws to compute predictive probabilities\n","\n","# kernel_posterior_scale_mean = -9.0  # Initial kernel posterior mean of the scale (log var) for q(w)\n","# kernel_posterior_scale_constraint =0.2  # kernel constraint for the scale (log var) of q(w).\")\n","\n","# kl_annealing = 50 #Epochs to anneal the KL term (anneals from 0 to 1)\n","\n","IMAGE_SHAPE = [28, 28, 1]\n","\n","model_dir = os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"),\n","                         \"bayesian_neural_network/\")  # Directory to put the model's fit\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t89CGIju4hWs","colab_type":"text"},"source":["## Input pipeline"]},{"cell_type":"code","metadata":{"id":"Izj7xM1-4gg8","colab_type":"code","colab":{}},"source":["def build_input_pipeline(x_train, x_test, y_train, y_test,\n","                         batch_size):\n","  \"\"\"Build an Iterator switching between train and heldout data.\"\"\"\n","  \n","  # Siccome questo dataset è piccolo posso permettermi di dare all'heldout \n","  # l'intero set come batch, inoltre non faccio la normalizzazione.\n","  valid_size = len(y_test)\n","\n","  # Converto in float e normalizzo\n","  x_train = x_train.astype(\"float32\")\n","  x_test = x_test.astype(\"float32\")\n","\n","  x_train /= 255\n","  x_test /= 255\n","\n","  print(\"x_train shape:\" + str(x_train.shape))\n","  print(str(x_train.shape[0]) + \" train samples\")\n","  print(str(x_test.shape[0]) + \" test samples\")\n","\n","  # Build an iterator over training batches.\n","  # Per avere poi la pmf ti serve che i labels siano int32\n","  training_dataset = tf.data.Dataset.from_tensor_slices(\n","      (x_train, np.int32(y_train)))\n","  # Non so se sovrascriverlo faccia la diff\n","  training_batches = training_dataset.shuffle(\n","      len(x_train), reshuffle_each_iteration=True).repeat().batch(batch_size)\n","  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n","\n","  ####################################################################################################\n","  # Non capisco perché 500 (è il valid size)... non dovrebbe essere len(x_test) ?\n","\n","  # Build a iterator over the heldout set with batch_size=heldout_size,\n","  # i.e., return the entire heldout set as a constant.\n","  heldout_dataset = tf.data.Dataset.from_tensor_slices((x_test, np.int32(y_test)))\n","  heldout_batches = heldout_dataset.repeat().batch(valid_size)  # niente reshuffle\n","  heldout_iterator = tf.compat.v1.data.make_one_shot_iterator(heldout_batches)\n","\n","  # Combine these into a feedable iterator that can switch between training\n","  # and validation inputs.\n","  handle = tf.compat.v1.placeholder(tf.string, shape=[])\n","  feedable_iterator = tf.compat.v1.data.Iterator.from_string_handle(\n","      handle, training_batches.output_types, training_batches.output_shapes)\n","  images, labels = feedable_iterator.get_next()\n","\n","  return images, labels, handle, training_iterator, heldout_iterator"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mJa2smRJ_Q8a","colab_type":"text"},"source":["## Main"]},{"cell_type":"code","metadata":{"id":"1AXYR6_p_RoV","colab_type":"code","outputId":"23f733c7-7ba6-453a-e8ae-6e109d7c7077","executionInfo":{"status":"ok","timestamp":1571664136525,"user_tz":-120,"elapsed":61746,"user":{"displayName":"Stefano Villata","photoUrl":"","userId":"07054100665848574913"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["with tf.name_scope(\"directory\"):\n","  if tf.io.gfile.exists(model_dir):\n","    tf.compat.v1.logging.warning(\n","        \"Warning: deleting old log directory at {}\".format(model_dir))\n","    tf.io.gfile.rmtree(model_dir)\n","  tf.io.gfile.makedirs(model_dir)\n","  \n","with tf.name_scope(\"Dataset\"):\n","  (images, labels, handle,\n","  training_iterator,\n","  heldout_iterator) = build_input_pipeline(x_train, x_val, y_train, y_val,\n","                                            batch_size)\n","\n","with tf.name_scope(\"BNN\"):\n","  inputs = tf.keras.layers.Input(shape=IMAGE_SHAPE, dtype='float32')\n","  x = tfp.layers.Convolution2DFlipout(32, (5, 5), activation='relu', padding='same')(inputs)\n","  x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n","  x = tfp.layers.Convolution2DFlipout(64, (5, 5), activation='relu', padding='same')(x)\n","  x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n","\n","  x = tf.keras.layers.Flatten()(x)\n","  x = tfp.layers.DenseFlipout(100, activation='relu')(x)\n","  outputs = tfp.layers.DenseFlipout(10, activation=None)(x)\n","  model = tf.keras.Model(inputs, outputs)\n","\n","logits = model(images)\n","\n","with tf.name_scope(\"loss\"):\n","  # Distribuzione categorica delle p per ogni istanza\n","  labels_distribution = tfd.Categorical(logits=logits)\n","\n","  # Perform KL annealing. The optimal number of annealing steps\n","  # depends on the dataset and architecture.\n","  # t = tf.compat.v2.Variable(0.0)  # Non ho idea del motivo per cui usi compat.v2, sembra però una normale variabile\n","  # kl_regularizer = t / (FLAGS.kl_annealing * len(x_train) / FLAGS.batch_size)\n","    \n","  \n","  # Compute the -ELBO as the loss. The kl term is annealed from 0 to 1 over\n","  # the epochs specified by the kl_annealing flag.\n","\n","  # Calcolare la neg log likelihood in questo modo oppure con sparse cross \n","  # entropy dà risultati simili, ma non uguali, probabilmente qualche differenza\n","  # è nella costruzione della distribuzione categorica ed estrazione della sua pmf\n","  log_likelihood = labels_distribution.log_prob(labels)  # PMF di ogni istanza\n","  neg_log_likelihood = -tf.reduce_mean(input_tensor=log_likelihood)\n","  kl = sum(model.losses) / len(x_train) # * tf.minimum(1.0, kl_regularizer)\n","  loss = neg_log_likelihood + kl\n","  \n","# Build metrics for evaluation. Predictions are formed from a single forward\n","# pass of the probabilistic layers. They are cheap but noisy\n","# predictions.\n","predictions = tf.argmax(input=logits, axis=1)\n","\n","with tf.name_scope(\"train\"):\n","  train_accuracy, train_accuracy_update_op = tf.metrics.accuracy(\n","      labels=labels, predictions=predictions)\n","  optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n","  train_op = optimizer.minimize(loss)\n","\n","with tf.name_scope(\"valid\"):\n","  valid_accuracy, valid_accuracy_update_op = tf.metrics.accuracy(\n","      labels=labels, predictions=predictions)\n","\n","init_op = tf.group(tf.global_variables_initializer(),\n","                  tf.local_variables_initializer())  # è un'op che esegue tutti i suoi input\n","\n","# Resetto le variabili che riguardano l'acc sul validation set, non voglio fare\n","# la media con la passata (epoca o iterazione?) precedente, ma le voglio ricalcolare\n","# da 0 per vedere come le performance sono cambiate\n","# Per resettare basta reinizializzarle\n","stream_vars_valid = [\n","    v for v in tf.local_variables() if \"valid/\" in v.name]\n","reset_valid_op = tf.compat.v1.variables_initializer(stream_vars_valid)\n","\n","saver = tf.train.Saver()  # Nodo di salvataggio\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["x_train shape:(51000, 28, 28, 1)\n","51000 train samples\n","9000 test samples\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-4mWx3yRK0KS","colab_type":"text"},"source":["## Execution phase"]},{"cell_type":"code","metadata":{"id":"OT-m8FXNK0Uo","colab_type":"code","outputId":"b5d38382-7972-4763-b8c1-8631c5d8959d","executionInfo":{"status":"error","timestamp":1571663350691,"user_tz":-120,"elapsed":274319,"user":{"displayName":"Stefano Villata","photoUrl":"","userId":"07054100665848574913"}},"colab":{"base_uri":"https://localhost:8080/","height":958}},"source":["with tf.Session() as sess:\n","  sess.run(init_op)\n","\n","  # Run the training loop\n","\n","  train_handle = sess.run(training_iterator.string_handle())\n","  heldout_handle = sess.run(heldout_iterator.string_handle())\n","  # Faccio girare tutto in modo da avere 1 epoca (len(x_train)/batch_size) per \n","  # il numero di epoche che voglio\n","  training_steps = int(\n","      round(epochs * (len(x_train) / batch_size)))\n","  \n","  # Non mi preoccupo del numero di step, tanto ho generato dati infiniti nel \n","  # dataset tramite repeat()\n","  for step in range(training_steps):\n","    _ = sess.run([train_op,\n","                  train_accuracy_update_op],\n","                  feed_dict={handle: train_handle})\n","\n","    # Manually print the frequency\n","    # ad ogni multiplo di 100 valuto loss, accuracy e kl\n","    if step % 100 == 0:\n","\n","      loss_value, accuracy_value, kl_value = sess.run(\n","            [loss, train_accuracy, kl], feed_dict={handle: train_handle})\n","      print(\n","            \"Step: {:>3d} Loss: {:.3f} Accuracy: {:.3f} KL: {:.3f}\".format(\n","                step, loss_value, accuracy_value, kl_value))\n","      save_path = saver.save(sess, \"/content/prova.ckpt\")\n","\n","\n","    if (step + 1) % eval_freq == 0:\n","    # Calculate validation accuracy\n","      sess.run(valid_accuracy_update_op, feed_dict={handle: heldout_handle})\n","      valid_value = sess.run(valid_accuracy, feed_dict={handle: heldout_handle})\n","\n","      print(\" ... Validation Accuracy: {:.3f}\".format(valid_value))\n","\n","      sess.run(reset_valid_op)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Step:   0 Loss: 24.168 Accuracy: 0.104 KL: 18.191\n","Step: 100 Loss: 20.222 Accuracy: 0.163 KL: 18.160\n","Step: 200 Loss: 19.250 Accuracy: 0.296 KL: 18.127\n","Step: 300 Loss: 18.797 Accuracy: 0.423 KL: 18.095\n"," ... Validation Accuracy: 0.816\n","Step: 400 Loss: 18.716 Accuracy: 0.514 KL: 18.061\n","Step: 500 Loss: 18.500 Accuracy: 0.579 KL: 18.027\n","Step: 600 Loss: 18.357 Accuracy: 0.626 KL: 17.992\n","Step: 700 Loss: 18.237 Accuracy: 0.663 KL: 17.956\n"," ... Validation Accuracy: 0.907\n","Step: 800 Loss: 18.267 Accuracy: 0.693 KL: 17.919\n","Step: 900 Loss: 18.107 Accuracy: 0.717 KL: 17.881\n","Step: 1000 Loss: 18.088 Accuracy: 0.737 KL: 17.843\n","Step: 1100 Loss: 17.989 Accuracy: 0.755 KL: 17.804\n"," ... Validation Accuracy: 0.939\n","Step: 1200 Loss: 17.875 Accuracy: 0.769 KL: 17.764\n","Step: 1300 Loss: 17.928 Accuracy: 0.782 KL: 17.723\n","Step: 1400 Loss: 17.840 Accuracy: 0.794 KL: 17.682\n","Step: 1500 Loss: 17.771 Accuracy: 0.804 KL: 17.640\n"," ... Validation Accuracy: 0.948\n","Step: 1600 Loss: 17.762 Accuracy: 0.813 KL: 17.597\n","Step: 1700 Loss: 17.670 Accuracy: 0.821 KL: 17.554\n","Step: 1800 Loss: 17.705 Accuracy: 0.828 KL: 17.511\n","Step: 1900 Loss: 17.589 Accuracy: 0.835 KL: 17.467\n"," ... Validation Accuracy: 0.963\n","Step: 2000 Loss: 17.589 Accuracy: 0.841 KL: 17.422\n","Step: 2100 Loss: 17.506 Accuracy: 0.847 KL: 17.377\n","Step: 2200 Loss: 17.433 Accuracy: 0.852 KL: 17.331\n","Step: 2300 Loss: 17.380 Accuracy: 0.857 KL: 17.285\n"," ... Validation Accuracy: 0.962\n","Step: 2400 Loss: 17.341 Accuracy: 0.861 KL: 17.238\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-3a6563a45729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     _ = sess.run([train_op,\n\u001b[1;32m     17\u001b[0m                   train_accuracy_update_op],\n\u001b[0;32m---> 18\u001b[0;31m                   feed_dict={handle: train_handle})\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Manually print the frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"egjy0yaMyLe_","colab_type":"text"},"source":["## emnist"]},{"cell_type":"code","metadata":{"id":"B3n2Zid7xOkq","colab_type":"code","colab":{}},"source":["x_train = emnist[\"dataset\"][0][0][0][0][0][0]\n","x_train = np.squeeze(x_train.astype(np.float32))\n","y_train = np.squeeze(emnist[\"dataset\"][0][0][0][0][0][1])\n","x_test = emnist[\"dataset\"][0][0][1][0][0][0]\n","x_test = x_test.astype(np.float32)\n","y_test = np.squeeze(emnist[\"dataset\"][0][0][1][0][0][1])\n","x_train /= 255\n","x_test /= 255\n","x_train = x_train.reshape(x_train.shape[0], 28, 28, order=\"A\")\n","x_test = x_test.reshape(x_test.shape[0], 28, 28, order=\"A\")\n","\n","# fondamentale normalizzare, siccome però è un dataset semplice non devi toccare\n","# la media, ma basta riportare tutto tra 0 e 1\n","\n","fake_labels = np.zeros((len(x_test), 10))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttHw37ahor00","colab_type":"text"},"source":["## Prove"]},{"cell_type":"code","metadata":{"id":"yfF3WkVMlDgv","colab_type":"code","outputId":"37524767-e955-4191-be30-be6b56fc1018","executionInfo":{"status":"ok","timestamp":1571664056913,"user_tz":-120,"elapsed":23648,"user":{"displayName":"Stefano Villata","photoUrl":"","userId":"07054100665848574913"}},"colab":{"base_uri":"https://localhost:8080/","height":126}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gab7i8KMla_6","colab_type":"code","colab":{}},"source":["!cp /content/prova.ckpt.data-00000-of-00001 gdrive/'My Drive'/Dataset_progetto\n","!cp /content/prova.ckpt.index gdrive/'My Drive'/Dataset_progetto\n","!cp /content/prova.ckpt.meta gdrive/'My Drive'/Dataset_progetto\n","!cp /content/checkpoint gdrive/'My Drive'/Dataset_progetto\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLg6emmqmSLo","colab_type":"code","outputId":"e52bc8ec-5890-4e03-cf41-655d9095387e","executionInfo":{"status":"ok","timestamp":1571664332088,"user_tz":-120,"elapsed":5194,"user":{"displayName":"Stefano Villata","photoUrl":"","userId":"07054100665848574913"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["with tf.Session() as sess:\n","  sess.run(init_op)\n","  saver.restore(sess, \"/content/gdrive/My Drive/Dataset_progetto/prova.ckpt\")\n","\n","  # Run the training loop\n","\n","  train_handle = sess.run(training_iterator.string_handle())\n","  heldout_handle = sess.run(heldout_iterator.string_handle())\n","  # Faccio girare tutto in modo da avere 1 epoca (len(x_train)/batch_size) per \n","  # il numero di epoche che voglio\n","  training_steps = int(\n","      round(epochs * (len(x_train) / batch_size)))\n","  \n","\n","    # Calculate validation accuracy\n","  sess.run(valid_accuracy_update_op, feed_dict={handle: heldout_handle})\n","  valid_value = sess.run(valid_accuracy, feed_dict={handle: heldout_handle})\n","\n","  print(\" ... Validation Accuracy: {:.3f}\".format(valid_value))\n","\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":[" ... Validation Accuracy: 0.963\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HPCG0p3In0i3","colab_type":"code","outputId":"c5907fea-1e0e-4f72-9304-e154d590ee5d","executionInfo":{"status":"ok","timestamp":1571663560098,"user_tz":-120,"elapsed":955,"user":{"displayName":"Stefano Villata","photoUrl":"","userId":"07054100665848574913"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#os.path.exists(content/gdrive/'My Drive'/Dataset_progetto/Saver)\n","os.path.exists(\"/content/gdrive/My Drive/Dataset_progetto/Saver\")\n","os.path.exists(\"/content/gdrive/My Drive/Dataset_progetto\")"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":12}]}]}